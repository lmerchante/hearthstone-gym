{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df8966f6",
   "metadata": {},
   "source": [
    "# Train your first Deep Reinforcement Learning Agent\n",
    "\n",
    "PYTHON 3.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9544e755",
   "metadata": {},
   "source": [
    "El objetivo de este notebook es crear un sistema para entrenar los algoritmos que sea claro y escalable. Así que voy a tratar el menor número de celdas posibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0c8944",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys #-- lo he necesitado para importar el paquete\n",
    "sys.path.append('..')\n",
    "import gym_hearthstone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f701d112",
   "metadata": {},
   "source": [
    "## Argumentos\n",
    "He añadido argumentos al environment para que sea más sencillo hacer el entrenamiento:\n",
    "\n",
    "    action_type: Define el espacio de acción que se va a crear. Ahora mismo solo hay tres implementados.\n",
    "    \n",
    "        * \"random\"  -> Solo una acción possible (Tomar una acción aleatoria de las que son posibles).\n",
    "        * \"type_rd\" -> 6 acciones posibles Realiza una acción aleatoria del tipo de acción devuelto por el DQN (en caso de tener alguna carta) y si no tengo ninguna carta del tipo de acción que ha devuelto el DQN entonces hace una acción aleatoria de entre las disponibles). NO TIENE SENTIDO PORQUE CONFUNDE AL DQN\n",
    "        * \"type\"    -> 6 acciones posibles (Realiza una acción aleatoria del tipo de acción devuelto por el DQN (en caso de tener alguna carta) y si no tengo ninguna carta del tipo de acción que ha devuelto el DQN no hace nada. A continuación vuelve a invocar al DQN para obtener otro tipo de acción).\n",
    "        \n",
    "        \n",
    "        \n",
    "    reward_mode: Define el reward que devuelve el environment:\n",
    "    \n",
    "        * \"simple\"    -> Solo hay reward cuando se gana o se pierde.\n",
    "        * \"penalize\"  -> Se penalizan las acciones que no son válidas.\n",
    "        * \"incentive\" -> Se premian las acciones que son válidas.\n",
    "        * \"complex\"   -> Se penalizan las acciones que no son válidas y se premian las válidas.\n",
    "\n",
    "\n",
    "    steps: Por ahora usa el numero de steps para el nombre del fichero de salida. \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ace088c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gym_hearthstone\n",
    "from stable_baselines3 import DQN\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc31913",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "steps = 100\n",
    "run = -1\n",
    "env = gym.make('Hearthstone-v1', action_type = \"random\", reward_mode = \"simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2117b05b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run += 1\n",
    "## Generar Modelo\n",
    "model = DQN(\"MultiInputPolicy\", env, verbose=1)\n",
    "\n",
    "# Resetear env y stats\n",
    "env.reset_stats()\n",
    "start = time.time()\n",
    "#env.reset()\n",
    "\n",
    "## Entrenar modelo\n",
    "model.learn(total_timesteps=steps, log_interval=4)\n",
    "end = time.time()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf22ddfa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Calcular estadísticas\n",
    "env.display_stats()\n",
    "\n",
    "total_time = end-start\n",
    "\n",
    "if(env.curr_episode > 0):\n",
    "    avg_time = (end-start) / env.curr_episode\n",
    "    avg_steps = steps / env.curr_episode\n",
    "else:\n",
    "    avg_time = (end-start) \n",
    "    avg_steps = steps \n",
    "\n",
    "\n",
    "if (env.wins + env.losses > 0):\n",
    "    win_rate = env.wins / (env.wins + env.losses)\n",
    "else:\n",
    "    win_rate = 0\n",
    "\n",
    "print(\"Avg steps per game: \" + str(avg_steps))\n",
    "print(\"Total running time: {:.2f} and running time per game: {:.2f}\".format(total_time, avg_time) )\n",
    "\n",
    "## Crear ficheros\n",
    "file_name = str(env.action_type) + \"__\" + str(env.reward_mode) + \"__\" + str(steps) + \"__\" + str(run)\n",
    "\n",
    "# METRICS\n",
    "data_file = open('./metrics/' + file_name + '.txt', 'w')\n",
    "data_file.write(\"Games, AvgStepsGame, TotalTime, AvgTimeGame, TotalReward, WinRate \\n\")\n",
    "data_file.write(\"{},{:.2f},{:.2f},{:.2f},{:.2f},{:.2f}\".format(env.curr_episode, avg_steps, total_time, avg_time, env.total_reward, win_rate))\n",
    "data_file.close()\n",
    "\n",
    "# REWARD DATA\n",
    "data_file = open('./data/' + file_name + '_reward.csv', 'w')\n",
    "data_file.write(\"Games, Reward \\n\")\n",
    "for key in env.reward_dict:\n",
    "    data_file.write(\"{},{:.2f} \\n\".format(key, env.reward_dict[key]))\n",
    "data_file.close()\n",
    "\n",
    "# Train Games\n",
    "data_file = open(\"./data/\" + file_name + '_games.csv', 'w')\n",
    "for l in env.games_outcome:\n",
    "    data_file.write(str(l) + \"\\n\")\n",
    "data_file.close()\n",
    "\n",
    "model.save(\"./dqn_models/\" + file_name + \"_dqn\")\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_gym-hearthstone",
   "language": "python",
   "name": "env_gym-hearthstone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
